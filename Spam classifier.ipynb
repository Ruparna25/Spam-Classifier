{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "72552fde-8989-49b5-9476-5e1a3982765c",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Spam Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "53221ed0-2c5a-4f6b-97cd-fecb4c19a217",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Download the data \n",
    "import os\n",
    "import tarfile\n",
    "import urllib \n",
    "\n",
    "DOWNLOAD_ROOT='https://spamassassin.apache.org/old/publiccorpus/'\n",
    "HAM_URL=DOWNLOAD_ROOT + '20030228_easy_ham.tar.bz2'\n",
    "SPAM_URL=DOWNLOAD_ROOT + '20030228_spam.tar.bz2'\n",
    "SPAM_PATH=os.path.join('datasets','spam')\n",
    "\n",
    "def fetch_spam_data(spam_url=SPAM_URL,spam_path=SPAM_PATH):\n",
    "    if not os.path.isdir(spam_path):\n",
    "        os.makedirs(spam_path)\n",
    "    for filename, url in ((\"ham.tar.bz2\", HAM_URL), (\"spam.tar.bz2\", SPAM_URL)):\n",
    "        file_path=os.path.join(spam_path + filename)\n",
    "        if not os.path.isfile(file_path):\n",
    "            urllib.request.urlretrieve(url,file_path)\n",
    "        tar_bz2_file=tarfile.open(file_path)   \n",
    "        tar_bz2_file.extractall(path=SPAM_PATH) \n",
    "        tar_bz2_file.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "7739d8a6-7725-40e9-9234-ad7c4c49011f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import tarfile\n",
    "import urllib\n",
    "\n",
    "DOWNLOAD_ROOT='https://spamassassin.apache.org/old/publiccorpus/'\n",
    "HAM_URL=DOWNLOAD_ROOT + '20030228_easy_ham.tar.bz2'\n",
    "SPAM_URL=DOWNLOAD_ROOT + '20030228_spam.tar.bz2'\n",
    "SPAM_PATH=os.path.join('datasets','spam')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "7184f9a2-9964-42d2-91ae-7cfcd62c6937",
   "metadata": {},
   "outputs": [],
   "source": [
    "#fetch_spam_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19926ad1-2d1a-4555-94eb-56cee29faaac",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "7bfa31cb-703b-484f-9b60-49a709bee1e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Extracting all ham and spam filenames\n",
    "ham_dir=os.path.join(SPAM_PATH, 'easy_ham')\n",
    "spam_dir=os.path.join(SPAM_PATH, 'spam')\n",
    "\n",
    "ham_filenames=[name for name in sorted(os.listdir(ham_dir)) if len(name) > 20]\n",
    "spam_filenames=[name for name in sorted(os.listdir(spam_dir)) if len(name) > 20]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "c3e5bd16-1e56-4c1b-974b-6092d2bc249f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2500"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(ham_filenames)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "aa1b080d-b579-4a09-9483-83cf744cbfd8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "500"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(spam_filenames)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "5c3886ca-dd39-44b9-95b2-369205700897",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Email Parser Function -\n",
    "import email\n",
    "import email.policy\n",
    "\n",
    "def load_email(is_spam, filename, spam_path=SPAM_PATH):\n",
    "    directory='spam' if is_spam else 'easy_ham'\n",
    "    with open(os.path.join(spam_path,directory,filename),'rb') as f:\n",
    "        return email.parser.BytesParser(policy=email.policy.default).parse(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "82df8f02-984e-486b-8ae3-944d949ccb3c",
   "metadata": {},
   "outputs": [],
   "source": [
    "#The output is a list of emails\n",
    "ham_emails=[load_email(is_spam=False, filename=name) for name in ham_filenames]\n",
    "spam_emails=[load_email(is_spam=True, filename=name) for name in spam_filenames]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "d4c50c8d-dc42-49c9-b6a4-ad439d3ba90a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Martin A posted:\n",
      "Tassos Papadopoulos, the Greek sculptor behind the plan, judged that the\n",
      " limestone of Mount Kerdylio, 70 miles east of Salonika and not far from the\n",
      " Mount Athos monastic community, was ideal for the patriotic sculpture. \n",
      " \n",
      " As well as Alexander's granite features, 240 ft high and 170 ft wide, a\n",
      " museum, a restored amphitheatre and car park for admiring crowds are\n",
      "planned\n",
      "---------------------\n",
      "So is this mountain limestone or granite?\n",
      "If it's limestone, it'll weather pretty fast.\n",
      "\n",
      "------------------------ Yahoo! Groups Sponsor ---------------------~-->\n",
      "4 DVDs Free +s&p Join Now\n",
      "http://us.click.yahoo.com/pt6YBB/NXiEAA/mG3HAA/7gSolB/TM\n",
      "---------------------------------------------------------------------~->\n",
      "\n",
      "To unsubscribe from this group, send an email to:\n",
      "forteana-unsubscribe@egroups.com\n",
      "\n",
      " \n",
      "\n",
      "Your use of Yahoo! Groups is subject to http://docs.yahoo.com/info/terms/\n"
     ]
    }
   ],
   "source": [
    "#Check the content of an email\n",
    "print(ham_emails[1].get_content().strip())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "2cb241d0-293f-4050-a4fd-5812aa6e4278",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<email.message.EmailMessage at 0x7f876c0e8f70>"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ham_emails[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "3148e694-995a-494e-b719-5c05d9921073",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1) Fight The Risk of Cancer!\n",
      "http://www.adclick.ws/p.cfm?o=315&s=pk007\n",
      "\n",
      "2) Slim Down - Guaranteed to lose 10-12 lbs in 30 days\n",
      "http://www.adclick.ws/p.cfm?o=249&s=pk007\n",
      "\n",
      "3) Get the Child Support You Deserve - Free Legal Advice\n",
      "http://www.adclick.ws/p.cfm?o=245&s=pk002\n",
      "\n",
      "4) Join the Web's Fastest Growing Singles Community\n",
      "http://www.adclick.ws/p.cfm?o=259&s=pk007\n",
      "\n",
      "5) Start Your Private Photo Album Online!\n",
      "http://www.adclick.ws/p.cfm?o=283&s=pk007\n",
      "\n",
      "Have a Wonderful Day,\n",
      "Offer Manager\n",
      "PrizeMama\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "If you wish to leave this list please use the link below.\n",
      "http://www.qves.com/trim/?ilug@linux.ie%7C17%7C114258\n",
      "\n",
      "\n",
      "-- \n",
      "Irish Linux Users' Group: ilug@linux.ie\n",
      "http://www.linux.ie/mailman/listinfo/ilug for (un)subscription information.\n",
      "List maintainer: listmaster@linux.ie\n"
     ]
    }
   ],
   "source": [
    "print(spam_emails[1].get_content().strip())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "64d7b74e-73e9-4d1e-a3bd-2227bfb58043",
   "metadata": {},
   "outputs": [],
   "source": [
    "#A lot of emails are of multiple parts so we need to seggregate the emails in one simple email\n",
    "def get_email_structure(email):\n",
    "    if isinstance(email, str):\n",
    "        return email\n",
    "    payload = email.get_payload()        \n",
    "    if isinstance(payload, list):\n",
    "        return \"multipart({})\".format(\", \".join([get_email_structure(sub_email) for sub_email in payload]))  #Joining all the subparts of the email\n",
    "    else:\n",
    "        return email.get_content_type()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "df539ba2-9c8e-4663-81fb-c1d373366f68",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Create a counter to see the different sub-parts of emails and which email contains what number of sub-parts\n",
    "from collections import Counter\n",
    "\n",
    "def structures_counter(emails):\n",
    "    structures = Counter()\n",
    "    for email in emails:\n",
    "        structure = get_email_structure(email)\n",
    "        structures[structure] += 1\n",
    "    return structures"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "07a9d365-8ce8-4c61-a5b2-01d6eef8461b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Counter({'text/plain': 2408,\n",
       "         'multipart(text/plain, application/pgp-signature)': 66,\n",
       "         'multipart(text/plain, text/html)': 8,\n",
       "         'multipart(text/plain, text/enriched)': 1,\n",
       "         'multipart(text/plain, application/ms-tnef, text/plain)': 1,\n",
       "         'multipart(text/plain)': 3,\n",
       "         'multipart(text/plain, application/octet-stream)': 2,\n",
       "         'multipart(text/plain, text/plain)': 4,\n",
       "         'multipart(multipart(text/plain, text/plain, text/plain), application/pgp-signature)': 1,\n",
       "         'multipart(text/plain, video/mng)': 1,\n",
       "         'multipart(text/plain, multipart(text/plain))': 1,\n",
       "         'multipart(text/plain, application/x-pkcs7-signature)': 1,\n",
       "         'multipart(text/plain, multipart(text/plain, text/plain), text/rfc822-headers)': 1,\n",
       "         'multipart(text/plain, multipart(text/plain, text/plain), multipart(multipart(text/plain, application/x-pkcs7-signature)))': 1,\n",
       "         'multipart(text/plain, application/x-java-applet)': 1})"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "structures_counter(ham_emails)                    #Observation of the ham emails - a lot of emails are plaintext emails"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "230bcfb3-71cf-4a60-91dc-7a089efa951e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Counter({'text/html': 183,\n",
       "         'text/plain': 218,\n",
       "         'multipart(text/plain, application/octet-stream)': 1,\n",
       "         'multipart(text/html)': 20,\n",
       "         'multipart(text/plain, text/html)': 45,\n",
       "         'multipart(text/plain)': 19,\n",
       "         'multipart(text/html, text/plain)': 1,\n",
       "         'multipart(text/html, application/octet-stream)': 2,\n",
       "         'multipart(multipart(text/html))': 5,\n",
       "         'multipart(text/plain, image/jpeg)': 3,\n",
       "         'multipart(multipart(text/html), application/octet-stream, image/jpeg)': 1,\n",
       "         'multipart(multipart(text/plain, text/html), image/gif)': 1,\n",
       "         'multipart/alternative': 1})"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "structures_counter(spam_emails)                       #A lot of email are htmls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "9bd43b4f-4438-4c54-8714-515b4dc68506",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Return-Path : <12a1mailbot1@web.de>\n",
      "Delivered-To : zzzz@localhost.spamassassin.taint.org\n",
      "Received : from localhost (localhost [127.0.0.1])\tby phobos.labs.spamassassin.taint.org (Postfix) with ESMTP id 136B943C32\tfor <zzzz@localhost>; Thu, 22 Aug 2002 08:17:21 -0400 (EDT)\n",
      "Received : from mail.webnote.net [193.120.211.219]\tby localhost with POP3 (fetchmail-5.9.0)\tfor zzzz@localhost (single-drop); Thu, 22 Aug 2002 13:17:21 +0100 (IST)\n",
      "Received : from dd_it7 ([210.97.77.167])\tby webnote.net (8.9.3/8.9.3) with ESMTP id NAA04623\tfor <zzzz@spamassassin.taint.org>; Thu, 22 Aug 2002 13:09:41 +0100\n",
      "From : 12a1mailbot1@web.de\n",
      "Received : from r-smtp.korea.com - 203.122.2.197 by dd_it7  with Microsoft SMTPSVC(5.5.1775.675.6);\t Sat, 24 Aug 2002 09:42:10 +0900\n",
      "To : dcek1a1@netsgo.com\n",
      "Subject : Life Insurance - Why Pay More?\n",
      "Date : Wed, 21 Aug 2002 20:31:57 -1600\n",
      "MIME-Version : 1.0\n",
      "Message-ID : <0103c1042001882DD_IT7@dd_it7>\n",
      "Content-Type : text/html; charset=\"iso-8859-1\"\n",
      "Content-Transfer-Encoding : quoted-printable\n"
     ]
    }
   ],
   "source": [
    "#Exploring the header of the spam email - items() function is used to extract each element in the list\n",
    "for hdr, value in spam_emails[0].items():\n",
    "    print(hdr,':',value)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "45a5f707-e2b5-4d89-b076-28acbc20143b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Life Insurance - Why Pay More?'"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spam_emails[0]['Subject']                  #We can explore each section of the email by calling out the name of the element, like we stated 'Subject' in this column"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "e5450ce0-b6b6-42d5-86aa-33eb5804101c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'12a1mailbot1@web.de'"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spam_emails[0]['From']                 #Just to validate I have tried to display the From field of the email in this block"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "id": "d6c8df62-b998-4188-afdb-d5e6472a3e57",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Dividing the data set in train and test data\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "X=np.array(ham_emails + spam_emails)\n",
    "y=np.array([0]*len(spam_emails) + [1]*len(ham_emails))\n",
    "\n",
    "X_train,X_test,y_train,y_test=train_test_split(X,y,test_size=0.2,random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "id": "f5909527-c390-4f61-ab03-5ed440280064",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Extract the plain text from the email - remove the html tags\n",
    "from bs4 import BeautifulSoup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "id": "6e5063f0-37a9-4003-8a43-9eb5eeab76d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def html_to_plain_text(html):\n",
    "  \n",
    "    soup = BeautifulSoup(html, \"html.parser\")\n",
    "    return '\\n'.join(soup.stripped_strings)        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "id": "7aa7b642-2ffd-412b-aafc-b981e6601a04",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Chuck Murcko wrote:\\n>[...stuff...]\\n\\nYawn.\\n\\nR\\n\\n\\n'"
      ]
     },
     "execution_count": 121,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train[0].get_content()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "id": "eec4a2eb-b1f7-4b48-9044-70fe18aa34a9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<HTML>\n",
      "<BODY BGCOLOR=\"#ffffff\">\n",
      "<P>\n",
      "<<HTML>\n",
      "<TABLE WIDTH=400 BORDER=0 CELLPADDING=0 CELLSPACING=0>\n",
      "  <TR>\n",
      "    <TD ALIGN=\"LEFT\" VALIGN=\"TOP\"><FONT FACE=\"Tahoma, Arial, Verdana\" SIZE=2></FONT>\n",
      "      <H2>\n",
      "\t<FONT COLOR=\"#FF0000\">GET HIGH...LEGALLY!</FONT>\n",
      "      </H2>\n",
      "      <P>\n",
      "      <B>IT REALLY WORKS!<BR>\n",
      "      PASSES ALL DRUG TESTS!<BR>\n",
      "      EXTREMELY POTENT!</B>\n",
      "      <P>\n",
      "      <A HREF=\"http://www.greenmatrix.net/herb/index.html\"><B>CLICK HERE for more\n",
      "      info on Salvia Divinorum</B></A>\n",
      "      <P>\n",
      "      <B> <A HREF=\"http://www.greenmatrix.net/herb/5x.html\">CLICK HERE for SALVIA\n",
      "      5X EXTRACT!</A> <BR>\n",
      "      <FONT COLOR=\"#FF0000\"><BIG>WARNING...VERY POTENT!</BIG></FONT></B>\n",
      "      <P>\n",
      "      <B> <A HREF=\"http://www.greenmatrix.net/herb/13x.html\">CLICK HERE for SALVIA\n",
      "      13X</A>. The most POTENT, LEGAL, SMOKABLE herb on the planet! 13 times the\n",
      "      power of Salvia Divinorum!<BR>\n",
      "      <FONT COLOR=\"#FF0000\"><BIG>WARNING...EXTREMELY POTENT!</BIG></FONT></B>\n",
      "      <P>\n",
      "      <P>\n",
      "    ...\n"
     ]
    }
   ],
   "source": [
    "#The emails of type text/html contain a lot of html details, below is a spam email's content \n",
    "html_spam_emails = [email for email in X_train[y_train==1]\n",
    "                    if get_email_structure(email) == \"text/html\"]\n",
    "sample_html_spam = html_spam_emails[0]\n",
    "print(sample_html_spam.get_content().strip()[:1000], \"...\")                    #Printing first 1000 characters of the email which is of type text/html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "id": "42f5ffb9-6cc9-43d7-a950-c0861d6e34d9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<\n",
      "GET HIGH...LEGALLY!\n",
      "IT REALLY WORKS!\n",
      "PASSES ALL DRUG TESTS!\n",
      "EXTREMELY POTENT!\n",
      "CLICK HERE for more\n",
      "      info on Salvia Divinorum\n",
      "CLICK HERE for SALVIA\n",
      "      5X EXTRACT!\n",
      "WARNING...VERY POTENT!\n",
      "CLICK HERE for SALVIA\n",
      "      13X\n",
      ". The most POTENT, LEGAL, SMOKABLE herb on the planet! 13 times the\n",
      "      power of Salvia Divinorum!\n",
      "WARNING...EXTREMELY POTENT!\n",
      "Removal Information:\n",
      "We are strongly against sending unsolicited emails to those who do not wish\n",
      "      to receive our special mailings. You have opted in to one or more of our\n",
      "      affiliate sites requesting to be notified of any special offers we may run\n",
      "      from time to time. This is NOT unsolicited email. If you do not wish to receive\n",
      "      further mailings, please\n",
      "click here to be removed\n",
      "      from the list\n",
      ". Please accept our apologies if you have been sent this\n",
      "      email in error. We honor all removal requests within 24 hours. ...\n"
     ]
    }
   ],
   "source": [
    "print(html_to_plain_text(sample_html_spam.get_content())[:1000], \"...\")         #Printing first 1000 characters of a spam email after converting to plain text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "id": "00a2c562-5e7e-456c-9b25-e3632dc6786b",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Writing a function which takes email as input and outputs its content as plain text - Not all email is having html tags so converting only htmls to plain text here\n",
    "def email_to_text(email):\n",
    "    html=None\n",
    "#The walk() method is an all-purpose generator which can be used to iterate over all the parts and subparts of a message object tree, in depth-first traversal order\n",
    "    for part in email.walk():\n",
    "        ctype=part.get_content_type()\n",
    "        if not ctype in ('text/plain','text/html'):\n",
    "            continue\n",
    "        try:\n",
    "            content=part.get_content()\n",
    "        except:\n",
    "            content=str(part.get_payload())\n",
    "        if ctype=='text/plain':\n",
    "            return content\n",
    "        else:\n",
    "            html=content\n",
    "    if html:\n",
    "        return html_to_plain_text(html)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "id": "be500dcb-5aa8-4539-ac10-f59f94bb9c0a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<\n",
      "GET HIGH...LEGALLY!\n",
      "IT REALLY WORKS!\n",
      "PASSES ALL DRUG TESTS!\n",
      "EXTREMELY POTENT!\n",
      "CLICK HERE for more\n",
      " ...\n"
     ]
    }
   ],
   "source": [
    "print(email_to_text(sample_html_spam)[:100], \"...\")              #The first 100character of the output of one last cell should be same as this"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "id": "f1a6aaeb-27a2-4378-a541-502ffb2d085d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Computations => comput\n",
      "Computation => comput\n",
      "Computing => comput\n",
      "Computed => comput\n",
      "Compute => comput\n",
      "Compulsive => compuls\n"
     ]
    }
   ],
   "source": [
    "#Introducting Stemming to make the word counting efficient, so that it counts the variations of the same word under one bracket\n",
    "import nltk\n",
    "\n",
    "stemmer=nltk.PorterStemmer()\n",
    "for word in (\"Computations\", \"Computation\", \"Computing\", \"Computed\", \"Compute\", \"Compulsive\"):\n",
    "        print(word, \"=>\", stemmer.stem(word))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "id": "935b752a-31a3-4136-99e9-e35aaae9143e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting nltk\n",
      "  Using cached nltk-3.6.5-py3-none-any.whl (1.5 MB)\n",
      "Collecting click\n",
      "  Using cached click-8.0.3-py3-none-any.whl (97 kB)\n",
      "Collecting joblib\n",
      "  Using cached joblib-1.1.0-py2.py3-none-any.whl (306 kB)\n",
      "Collecting regex>=2021.8.3\n",
      "  Using cached regex-2021.10.21-cp38-cp38-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (764 kB)\n",
      "Collecting tqdm\n",
      "  Using cached tqdm-4.62.3-py2.py3-none-any.whl (76 kB)\n",
      "Installing collected packages: tqdm, regex, joblib, click, nltk\n",
      "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
      "dc-workspace 1.0.0 requires jupytext, which is not installed.\u001b[0m\n",
      "Successfully installed click-8.0.3 joblib-1.1.0 nltk-3.6.5 regex-2021.10.21 tqdm-4.62.3\n",
      "\u001b[33mWARNING: Target directory /home/repl/.local/lib/python3.8/site-packages/regex-2021.10.21.dist-info already exists. Specify --upgrade to force replacement.\u001b[0m\n",
      "\u001b[33mWARNING: Target directory /home/repl/.local/lib/python3.8/site-packages/joblib already exists. Specify --upgrade to force replacement.\u001b[0m\n",
      "\u001b[33mWARNING: Target directory /home/repl/.local/lib/python3.8/site-packages/click already exists. Specify --upgrade to force replacement.\u001b[0m\n",
      "\u001b[33mWARNING: Target directory /home/repl/.local/lib/python3.8/site-packages/joblib-1.1.0.dist-info already exists. Specify --upgrade to force replacement.\u001b[0m\n",
      "\u001b[33mWARNING: Target directory /home/repl/.local/lib/python3.8/site-packages/nltk already exists. Specify --upgrade to force replacement.\u001b[0m\n",
      "\u001b[33mWARNING: Target directory /home/repl/.local/lib/python3.8/site-packages/nltk-3.6.5.dist-info already exists. Specify --upgrade to force replacement.\u001b[0m\n",
      "\u001b[33mWARNING: Target directory /home/repl/.local/lib/python3.8/site-packages/regex already exists. Specify --upgrade to force replacement.\u001b[0m\n",
      "\u001b[33mWARNING: Target directory /home/repl/.local/lib/python3.8/site-packages/tqdm-4.62.3.dist-info already exists. Specify --upgrade to force replacement.\u001b[0m\n",
      "\u001b[33mWARNING: Target directory /home/repl/.local/lib/python3.8/site-packages/tqdm already exists. Specify --upgrade to force replacement.\u001b[0m\n",
      "\u001b[33mWARNING: Target directory /home/repl/.local/lib/python3.8/site-packages/click-8.0.3.dist-info already exists. Specify --upgrade to force replacement.\u001b[0m\n",
      "\u001b[33mWARNING: Target directory /home/repl/.local/lib/python3.8/site-packages/bin already exists. Specify --upgrade to force replacement.\u001b[0m\n",
      "\u001b[33mWARNING: You are using pip version 21.2.4; however, version 21.3 is available.\n",
      "You should consider upgrading via the '/usr/bin/python3 -m pip install --upgrade pip' command.\u001b[0m\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install nltk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "id": "709cd470-093f-4555-a18c-dd653ea8db66",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to /home/repl/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "#Removing Stopwords -\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "nltk.download('stopwords')\n",
    "stop_words = set(stopwords.words('english'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "id": "024f9154-5411-468c-9d89-aa6ad36d1592",
   "metadata": {},
   "outputs": [],
   "source": [
    "#stop_words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "id": "78905af1-836d-4c50-bf46-962b92ce616f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting urlextract\n",
      "  Using cached urlextract-1.4.0-py3-none-any.whl (20 kB)\n",
      "Collecting uritools\n",
      "  Using cached uritools-3.0.2-py3-none-any.whl (12 kB)\n",
      "Collecting idna\n",
      "  Using cached idna-3.3-py3-none-any.whl (61 kB)\n",
      "Collecting appdirs\n",
      "  Using cached appdirs-1.4.4-py2.py3-none-any.whl (9.6 kB)\n",
      "Collecting filelock\n",
      "  Using cached filelock-3.3.1-py3-none-any.whl (9.7 kB)\n",
      "Installing collected packages: uritools, idna, filelock, appdirs, urlextract\n",
      "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
      "dc-workspace 1.0.0 requires jupytext, which is not installed.\u001b[0m\n",
      "Successfully installed appdirs-1.4.4 filelock-3.3.1 idna-3.3 uritools-3.0.2 urlextract-1.4.0\n",
      "\u001b[33mWARNING: Target directory /home/repl/.local/lib/python3.8/site-packages/appdirs.py already exists. Specify --upgrade to force replacement.\u001b[0m\n",
      "\u001b[33mWARNING: Target directory /home/repl/.local/lib/python3.8/site-packages/urlextract-1.4.0.dist-info already exists. Specify --upgrade to force replacement.\u001b[0m\n",
      "\u001b[33mWARNING: Target directory /home/repl/.local/lib/python3.8/site-packages/appdirs-1.4.4.dist-info already exists. Specify --upgrade to force replacement.\u001b[0m\n",
      "\u001b[33mWARNING: Target directory /home/repl/.local/lib/python3.8/site-packages/urlextract already exists. Specify --upgrade to force replacement.\u001b[0m\n",
      "\u001b[33mWARNING: Target directory /home/repl/.local/lib/python3.8/site-packages/idna-3.3.dist-info already exists. Specify --upgrade to force replacement.\u001b[0m\n",
      "\u001b[33mWARNING: Target directory /home/repl/.local/lib/python3.8/site-packages/filelock already exists. Specify --upgrade to force replacement.\u001b[0m\n",
      "\u001b[33mWARNING: Target directory /home/repl/.local/lib/python3.8/site-packages/filelock-3.3.1.dist-info already exists. Specify --upgrade to force replacement.\u001b[0m\n",
      "\u001b[33mWARNING: Target directory /home/repl/.local/lib/python3.8/site-packages/__pycache__ already exists. Specify --upgrade to force replacement.\u001b[0m\n",
      "\u001b[33mWARNING: Target directory /home/repl/.local/lib/python3.8/site-packages/uritools-3.0.2.dist-info already exists. Specify --upgrade to force replacement.\u001b[0m\n",
      "\u001b[33mWARNING: Target directory /home/repl/.local/lib/python3.8/site-packages/idna already exists. Specify --upgrade to force replacement.\u001b[0m\n",
      "\u001b[33mWARNING: Target directory /home/repl/.local/lib/python3.8/site-packages/uritools already exists. Specify --upgrade to force replacement.\u001b[0m\n",
      "\u001b[33mWARNING: Target directory /home/repl/.local/lib/python3.8/site-packages/bin already exists. Specify --upgrade to force replacement.\u001b[0m\n",
      "\u001b[33mWARNING: You are using pip version 21.2.4; however, version 21.3 is available.\n",
      "You should consider upgrading via the '/usr/bin/python3 -m pip install --upgrade pip' command.\u001b[0m\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install urlextract"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "id": "496336d5-fdd2-472a-adea-f5e089b4e14d",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Used for extracting all the urls of the email, so that we can remove them when building our word count matrix\n",
    "import urlextract\n",
    "\n",
    "url_extractor=urlextract.URLExtract()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "id": "10045009-0c51-40ed-92d8-a74b772d7514",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Now that we have extracted all the data from email - lets break down the sentences into word and check the frequency of each word\n",
    "import re\n",
    "from sklearn.base import BaseEstimator, TransformerMixin\n",
    "\n",
    "class EmailToWordCounterTransform(BaseEstimator, TransformerMixin):\n",
    "    def __init__(self, strip_headers=True, lower_case=True, remove_punctuation=True, replace_urls=True, replace_numbers=True, stemming=True, remove_stopwords=True):\n",
    "        self.strip_headers=strip_headers\n",
    "        self.lower_case=lower_case\n",
    "        self.remove_punctuation=remove_punctuation\n",
    "        self.replace_urls=replace_urls\n",
    "        self.replace_numbers=replace_numbers\n",
    "        self.stemming=stemming\n",
    "        self.remove_stopwords=remove_stopwords\n",
    "    def fit(self, X, y=None):\n",
    "        return self\n",
    "    def transform(self, X, y=None):\n",
    "        X_transformed=[]\n",
    "        for email in X:\n",
    "            text = email_to_text(email) or \"\"\n",
    "            if self.lower_case:\n",
    "                text=text.lower()\n",
    "            if self.replace_urls and url_extractor is not None:\n",
    "                urls=list(set(url_extractor.find_urls(text)))           #Set function returns an empty set if no element is passed. Non-repeating element iterable modified as passed as argument.\n",
    "                urls.sort(key=lambda url: len(url), reverse=True)\n",
    "                for url in urls:\n",
    "                    text=text.replace(url, ' URL ')\n",
    "            if self.replace_numbers:\n",
    "                text=re.sub(r'\\d+(?:\\.\\d*(?:[eE]\\d+))?','NUMBER', text)\n",
    "            if self.remove_punctuation:\n",
    "                text=re.sub(r'\\W+',' ',text,flags=re.M)\n",
    "            if self.remove_stopwords:\n",
    "                words=text.split()\n",
    "                words_without_sw=[word for word in words if not word in stop_words]\n",
    "            word_counts=Counter(words_without_sw)\n",
    "            if self.stemming and stemmer is not None:\n",
    "                stemmed_word_counts=Counter()\n",
    "                for word,count in word_counts.items():\n",
    "                    stemmed_word = stemmer.stem(word)\n",
    "                    stemmed_word_counts[stemmed_word] += count\n",
    "                word_counts = stemmed_word_counts\n",
    "            X_transformed.append(word_counts)    \n",
    "        return np.array(X_transformed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "id": "9cb81308-a2bb-4ceb-8834-5f28d122d433",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([Counter({'chuck': 1, 'murcko': 1, 'wrote': 1, 'stuff': 1, 'yawn': 1, 'r': 1}),\n",
       "       Counter({'christian': 3, 'jefferson': 2, 'superstit': 2, 'one': 2, 'half': 2, 'rogueri': 2, 'teach': 2, 'jesu': 2, 'interest': 1, 'quot': 1, 'url': 1, 'thoma': 1, 'examin': 1, 'known': 1, 'word': 1, 'find': 1, 'particular': 1, 'redeem': 1, 'featur': 1, 'alik': 1, 'found': 1, 'fabl': 1, 'mytholog': 1, 'million': 1, 'innoc': 1, 'men': 1, 'women': 1, 'children': 1, 'sinc': 1, 'introduct': 1, 'burnt': 1, 'tortur': 1, 'fine': 1, 'imprison': 1, 'effect': 1, 'coercion': 1, 'make': 1, 'world': 1, 'fool': 1, 'hypocrit': 1, 'support': 1, 'error': 1, 'earth': 1, 'six': 1, 'histor': 1, 'american': 1, 'john': 1, 'e': 1, 'remsburg': 1, 'letter': 1, 'william': 1, 'short': 1, 'becom': 1, 'pervert': 1, 'system': 1, 'ever': 1, 'shone': 1, 'man': 1, 'absurd': 1, 'untruth': 1, 'perpetr': 1, 'upon': 1, 'larg': 1, 'band': 1, 'dupe': 1, 'import': 1, 'led': 1, 'paul': 1, 'first': 1, 'great': 1, 'corrupt': 1}),\n",
       "       Counter({'url': 4, 'group': 3, 'forteana': 2, 'martin': 2, 'yahoo': 2, 'unsubscrib': 2, 'adamson': 1, 'wrote': 1, 'altern': 1, 'rather': 1, 'factual': 1, 'base': 1, 'rundown': 1, 'hamza': 1, 'career': 1, 'includ': 1, 'belief': 1, 'non': 1, 'muslim': 1, 'yemen': 1, 'murder': 1, 'outright': 1, 'know': 1, 'unbias': 1, 'memri': 1, 'html': 1, 'rob': 1, 'sponsor': 1, 'number': 1, 'dvd': 1, 'free': 1, 'p': 1, 'join': 1, 'send': 1, 'email': 1, 'egroup': 1, 'com': 1, 'use': 1, 'subject': 1})],\n",
       "      dtype=object)"
      ]
     },
     "execution_count": 133,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_few = X_train[:3]\n",
    "X_few_wordcounts = EmailToWordCounterTransform().fit_transform(X_few)\n",
    "X_few_wordcounts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "id": "755a74b6-3250-47de-9652-c1454da190d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Creating vocabulary as a feature matrix\n",
    "from scipy.sparse import csr_matrix\n",
    "\n",
    "class WordCounterToVectorTransform(BaseEstimator, TransformerMixin):\n",
    "    def __init__(self, vocabulary_size=1000):\n",
    "        self.vocabulary_size=vocabulary_size\n",
    "    def fit(self,X,y=None):\n",
    "        total_count=Counter()                                         \n",
    "        for word_count in X:                              \n",
    "            for word, count in word_count.items():\n",
    "                total_count[word] += min(count, 10)                     #Finding out common words which has occurances less than equal to 10\n",
    "        most_common=total_count.most_common()[:self.vocabulary_size]    #Finding out the most common words to build the vocabulary array\n",
    "        self.most_common_=most_common                                   #Assigning to an instance of the class\n",
    "        self.vocabulary_={word: index + 1 for index, (word, count) in enumerate(most_common)}      #Setting up the vocab - it will have a combination of word & index\n",
    "        print(self.vocabulary_)                                         #Assigning to another instance of the class\n",
    "        return self\n",
    "    def transform(self,X,y=None):\n",
    "        rows=[]\n",
    "        cols=[]\n",
    "        data=[]\n",
    "        for row, word_count in enumerate(X):                          \n",
    "            for word, count in word_count.items():                   #Checking the number of words present in the each email\n",
    "                rows.append(row)                                     #Building the rows of the matrix\n",
    "                cols.append(self.vocabulary_.get(word, 0))           #Storing the index of the word present in the vocabulary to be used to build csr matrix\n",
    "                data.append(count)\n",
    "            #print(rows[7:])\n",
    "            #print(cols[7:])\n",
    "            #print(data)\n",
    "        return csr_matrix((data, (rows, cols)), shape=(len(X), self.vocabulary_size + 1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "id": "22d38aac-8f71-4c65-8fdc-bb40e31c598d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'url': 1, 'christian': 2, 'group': 3, 'wrote': 4, 'jefferson': 5, 'superstit': 6, 'one': 7, 'half': 8, 'rogueri': 9, 'teach': 10}\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<3x11 sparse matrix of type '<class 'numpy.int64'>'\n",
       "\twith 15 stored elements in Compressed Sparse Row format>"
      ]
     },
     "execution_count": 135,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vocab_transformer = WordCounterToVectorTransform(vocabulary_size=10)\n",
    "X_few_vectors = vocab_transformer.fit_transform(X_few_wordcounts)\n",
    "X_few_vectors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "id": "c975306f-7817-444a-a851-52ceafb9bb8a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 5,  0,  0,  0,  1,  0,  0,  0,  0,  0,  0],\n",
       "       [64,  1,  3,  0,  0,  2,  2,  2,  2,  2,  2],\n",
       "       [40,  4,  0,  3,  1,  0,  0,  0,  0,  0,  0]])"
      ]
     },
     "execution_count": 136,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_few_vectors.toarray()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "id": "caec1423-cdff-4094-983a-3086f9d2c776",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'url': 1,\n",
       " 'christian': 2,\n",
       " 'group': 3,\n",
       " 'wrote': 4,\n",
       " 'jefferson': 5,\n",
       " 'superstit': 6,\n",
       " 'one': 7,\n",
       " 'half': 8,\n",
       " 'rogueri': 9,\n",
       " 'teach': 10}"
      ]
     },
     "execution_count": 137,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vocab_transformer.vocabulary_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "id": "e25be181-e3a1-4a75-a716-880ec9a96478",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'number': 1, 'url': 2, 'list': 3, 'use': 4, 'get': 5, 'mail': 6, 'one': 7, 'time': 8, 'like': 9, 'messag': 10, 'com': 11, 'email': 12, 'would': 13, 'work': 14, 'make': 15, 'new': 16, 'peopl': 17, 'free': 18, 'user': 19, 'net': 20, 'date': 21, 'wrote': 22, 'rpm': 23, 'want': 24, 'linux': 25, 'also': 26, 'spamassassin': 27, 'file': 28, 'go': 29, 'need': 30, 'year': 31, 'way': 32, 'think': 33, 'group': 34, 'tri': 35, 'look': 36, 'receiv': 37, 'know': 38, 'us': 39, 'e': 40, '_______________________________________________': 41, 'run': 42, 'inform': 43, 'even': 44, 'spam': 45, 'said': 46, 'could': 47, 'first': 48, 'say': 49, 'see': 50, 'chang': 51, 'right': 52, 'problem': 53, 'exmh': 54, 'pleas': 55, 'world': 56, 'line': 57, 'day': 58, 'thing': 59, 'take': 60, 'may': 61, 'send': 62, 'money': 63, 'system': 64, 'remov': 65, 'find': 66, 'good': 67, 'much': 68, 'well': 69, 'name': 70, 'compani': 71, 'call': 72, 'mani': 73, 'help': 74, 'talk': 75, 'servic': 76, 'subject': 77, 'packag': 78, 'address': 79, 'click': 80, 'start': 81, 'includ': 82, 'internet': 83, 'busi': 84, 'give': 85, 'link': 86, 'state': 87, 'home': 88, 'set': 89, 'instal': 90, 'still': 91, 'someth': 92, 'best': 93, 'two': 94, 'come': 95, 'gener': 96, 'differ': 97, 'site': 98, 'unsubscrib': 99, 'comput': 100, 'market': 101, 'realli': 102, 'phone': 103, 'web': 104, 'seem': 105, 'write': 106, 'c': 107, 'data': 108, 'softwar': 109, 'program': 110, 'read': 111, 'check': 112, 'person': 113, 'point': 114, 'ie': 115, 'interest': 116, 'sure': 117, 'old': 118, 'found': 119, 'back': 120, 'order': 121, 'base': 122, 'sep': 123, 'report': 124, 'made': 125, 'million': 126, 'offer': 127, 'everi': 128, 'sponsor': 129, 'current': 130, 'sinc': 131, 'last': 132, 'server': 133, 'case': 134, 'today': 135, 'numbertnumb': 136, 'actual': 137, 'provid': 138, 'end': 139, 'commun': 140, 'fork': 141, 'follow': 142, 'secur': 143, 'next': 144, 'origin': 145, 'mean': 146, 'anoth': 147, 'test': 148, 'code': 149, 'got': 150, 'perl': 151, 'r': 152, 'without': 153, 'x': 154, 'long': 155, 'never': 156, 'live': 157, 'put': 158, 'better': 159, 'razor': 160, 'anyon': 161, 'real': 162, 'thank': 163, 'lot': 164, 'sent': 165, 'part': 166, 'place': 167, 'manag': 168, 'might': 169, 'ad': 170, 'month': 171, 'sourceforg': 172, 'life': 173, 'keep': 174, 'folder': 175, 'idea': 176, 'error': 177, 'form': 178, 'network': 179, 'build': 180, 'version': 181, 'week': 182, 'within': 183, 'type': 184, 'invest': 185, 'requir': 186, 'probabl': 187, 'govern': 188, 'rate': 189, 'countri': 190, 'show': 191, 'product': 192, 'support': 193, 'yahoo': 194, 'post': 195, 'price': 196, 'issu': 197, 'discuss': 198, 'someon': 199, 'mailto': 200, 'window': 201, 'high': 202, 'reason': 203, 'org': 204, 'sourc': 205, 'custom': 206, 'suppli': 207, 'releas': 208, 'page': 209, 'word': 210, 'develop': 211, 'easi': 212, 'must': 213, 'move': 214, 'redhat': 215, 'let': 216, 'anyth': 217, 'result': 218, 'text': 219, 'etc': 220, 'less': 221, 'freshrpm': 222, 'plan': 223, 'past': 224, 'septemb': 225, 'around': 226, 'per': 227, 'content': 228, 'train': 229, 'total': 230, 'access': 231, 'exampl': 232, 'news': 233, 'done': 234, 'f': 235, 'ye': 236, 'allow': 237, 'great': 238, 'key': 239, 'share': 240, 'public': 241, 'though': 242, 'pay': 243, 'tell': 244, 'wish': 245, 'matthia': 246, 'cours': 247, 'maintain': 248, 'add': 249, 'hour': 250, 'sell': 251, 'save': 252, 'happen': 253, 'kernel': 254, 'avail': 255, 'question': 256, 'open': 257, 'ever': 258, 'possibl': 259, 'second': 260, 'engin': 261, 'log': 262, 'power': 263, 'least': 264, 'instead': 265, 'els': 266, 'header': 267, 'ask': 268, 'red': 269, 'littl': 270, 'buy': 271, 'account': 272, 'stuff': 273, 'prefer': 274, 'believ': 275, 'futur': 276, 'creat': 277, 'rule': 278, 'experi': 279, 'html': 280, 'applic': 281, 'exist': 282, 'polit': 283, 'local': 284, 'american': 285, 'law': 286, 'recent': 287, 'updat': 288, 'success': 289, 'complet': 290, 'process': 291, 'mayb': 292, 'un': 293, 'friend': 294, 'technolog': 295, 'fact': 296, 'import': 297, 'special': 298, 'yet': 299, 'enough': 300, 'howev': 301, 'contact': 302, 'w': 303, 'b': 304, 'deal': 305, 'l': 306, 'thought': 307, 'bit': 308, 'becom': 309, 'u': 310, 'big': 311, 'pm': 312, 'noth': 313, 'search': 314, 'stori': 315, 'select': 316, 'subscript': 317, 'featur': 318, 'hand': 319, 'control': 320, 'far': 321, 'valu': 322, 'ham': 323, 'industri': 324, 'non': 325, 'larg': 326, 'alway': 327, 'client': 328, 'apt': 329, 'hat': 330, 'term': 331, 'alsa': 332, 'p': 333, 'cost': 334, 'offic': 335, 'kind': 336, 'learn': 337, 'alreadi': 338, 'dollar': 339, 'thu': 340, 'trade': 341, 'protect': 342, 'note': 343, 'stop': 344, 'onlin': 345, 'play': 346, 'turn': 347, 'irish': 348, 'forward': 349, 'n': 350, 'admin': 351, 'nation': 352, 'design': 353, 'thousand': 354, 'citi': 355, 'book': 356, 'hit': 357, 'copi': 358, 'simpl': 359, 'understand': 360, 'feel': 361, 'love': 362, 'cd': 363, 'advertis': 364, 'other': 365, 'respons': 366, '_': 367, 'effect': 368, 'directori': 369, 'record': 370, 'fix': 371, 'job': 372, 'man': 373, 'act': 374, 'aug': 375, 'score': 376, 'financi': 377, 'sound': 378, 'opportun': 379, 'seen': 380, 'either': 381, 'load': 382, 'ilug': 383, 'unit': 384, 'famili': 385, 'card': 386, 'root': 387, 'increas': 388, 'dvd': 389, 'three': 390, 'mr': 391, 'away': 392, 'legal': 393, 'incom': 394, 'limit': 395, 'fund': 396, 'databas': 397, 'box': 398, 'sequenc': 399, 'hope': 400, 'begin': 401, 'usr': 402, 'hard': 403, 'member': 404, 'full': 405, 'appear': 406, 'claim': 407, 'top': 408, 'ok': 409, 'forc': 410, 'corpor': 411, 'return': 412, 'answer': 413, 'true': 414, 'expect': 415, 'join': 416, 'posit': 417, 'standard': 418, 'mind': 419, 'repli': 420, 'presid': 421, 'step': 422, 'rather': 423, 'wrong': 424, 'connect': 425, 'level': 426, 'g': 427, 'bill': 428, 'abl': 429, 'caus': 430, 'often': 431, 'care': 432, 'center': 433, 'small': 434, 'direct': 435, 'profession': 436, 'drive': 437, 'cell': 438, 'rememb': 439, 'listmast': 440, 'j': 441, 'major': 442, 'machin': 443, 'signatur': 444, 'oper': 445, 'mark': 446, 'quit': 447, 'specif': 448, 'partner': 449, 'regard': 450, 'chri': 451, 'ago': 452, 'perform': 453, 'accept': 454, 'notic': 455, 'size': 456, 'war': 457, 'game': 458, 'forteana': 459, 'wed': 460, 'depend': 461, 'individu': 462, 'cash': 463, 'digit': 464, 'wonder': 465, 'continu': 466, 'whether': 467, 'project': 468, 'everyth': 469, 'bad': 470, 'sever': 471, 'bank': 472, 'univers': 473, 'devel': 474, 'sort': 475, 'tool': 476, 'risk': 477, 'guarante': 478, 'contain': 479, 'welcom': 480, 'immedi': 481, 'area': 482, 'given': 483, 'copyright': 484, 'parti': 485, 'info': 486, 'figur': 487, 'bug': 488, 'quot': 489, 'polici': 490, 'subscrib': 491, 'singl': 492, 'wait': 493, 'lost': 494, 'assist': 495, 'xent': 496, 'upon': 497, 'women': 498, 'clean': 499, 'grow': 500, 'geek': 501, 'websit': 502, 'request': 503, 'sign': 504, 'intern': 505, 'script': 506, 'lib': 507, 'anyway': 508, 'simpli': 509, 'comment': 510, 'half': 511, 'fail': 512, 'visit': 513, 'document': 514, 'pretti': 515, 'privat': 516, 'entir': 517, 'econom': 518, 'america': 519, 'fals': 520, 'whole': 521, 'meet': 522, 'execut': 523, 'imag': 524, 'option': 525, 'view': 526, 'decid': 527, 'java': 528, 'nice': 529, 'publish': 530, 'pick': 531, 'command': 532, 'upgrad': 533, 'matter': 534, 'later': 535, 'transact': 536, 'ship': 537, 'worker': 538, 'detail': 539, 'msg': 540, 'egroup': 541, 'almost': 542, 'tire': 543, 'h': 544, 'guy': 545, 'august': 546, 'went': 547, 'john': 548, 'space': 549, 'solut': 550, 'store': 551, 'addit': 552, 'similar': 553, 'via': 554, 'stock': 555, 'method': 556, 'agre': 557, 'suggest': 558, 'practic': 559, 'low': 560, 'transfer': 561, 'came': 562, 'fast': 563, 'final': 564, 'agent': 565, 'amount': 566, 'benefit': 567, 'except': 568, 'school': 569, 'fax': 570, 'attack': 571, 'discov': 572, 'easili': 573, 'research': 574, 'relat': 575, 'minut': 576, 'global': 577, 'societi': 578, 'regist': 579, 'author': 580, 'exactli': 581, 'languag': 582, 'told': 583, 'interact': 584, 'although': 585, 'close': 586, 'modul': 587, 'sex': 588, 'collect': 589, 'instruct': 590, 'happi': 591, 'respect': 592, 'social': 593, 'watch': 594, 'distribut': 595, 'hi': 596, 'articl': 597, 'memori': 598, 'activ': 599, 'consid': 600, 'pass': 601, 'opt': 602, 'late': 603, 'profit': 604, 'driver': 605, 'street': 606, 'secret': 607, 'domain': 608, 'feder': 609, 'python': 610, 'sa': 611, 'mon': 612, 'cut': 613, 'bodi': 614, 'fall': 615, 'lead': 616, 'daili': 617, 'left': 618, 'due': 619, 'hundr': 620, 'present': 621, 'filter': 622, 'head': 623, 'associ': 624, 'propos': 625, 'bush': 626, 'everyon': 627, 'cv': 628, 'guess': 629, 'heaven': 630, 'improv': 631, 'assum': 632, 'involv': 633, 'monday': 634, 'spammer': 635, 'known': 636, 'face': 637, 'effort': 638, 'natur': 639, 'de': 640, 'reach': 641, 'insur': 642, 'die': 643, 'token': 644, 'perhap': 645, 'download': 646, 'procmail': 647, 'basic': 648, 'appli': 649, 'worth': 650, 'leav': 651, 'hous': 652, 'remain': 653, 'clear': 654, 'action': 655, 'car': 656, 'miss': 657, 'fine': 658, 'choic': 659, 'explain': 660, 'normal': 661, 'octob': 662, 'enabl': 663, 'human': 664, 'men': 665, 'tim': 666, 'fill': 667, 'googl': 668, 'offici': 669, 'absolut': 670, 'inc': 671, 'tax': 672, 'deliv': 673, 'suit': 674, 'target': 675, 'osdn': 676, 'os': 677, 'credit': 678, 'devic': 679, 'platform': 680, 'thinkgeek': 681, 'charg': 682, 'averag': 683, 'paper': 684, 'beberg': 685, 'plu': 686, 'age': 687, 'robert': 688, 'spend': 689, 'sale': 690, 'statement': 691, 'administr': 692, 'altern': 693, 'fri': 694, 'main': 695, 'saou': 696, 'trust': 697, 'hettinga': 698, 'usa': 699, 'edit': 700, 'purchas': 701, 'delet': 702, 'numberpm': 703, 'self': 704, 'york': 705, 'unseen': 706, 'print': 707, 'ignor': 708, 'refer': 709, 'press': 710, 'port': 711, 'five': 712, 'rah': 713, 'electron': 714, 'suppos': 715, 'goe': 716, 'across': 717, 'contract': 718, 'speed': 719, 'along': 720, 'commiss': 721, 'particular': 722, 'integr': 723, 'cannot': 724, 'paid': 725, 'kill': 726, 'usual': 727, 'oct': 728, 'newslett': 729, 'organ': 730, 'inumb': 731, 'skip': 732, 'dave': 733, 'tag': 734, 'side': 735, 'hold': 736, 'track': 737, 'sun': 738, 'commit': 739, 'studi': 740, 'capit': 741, 'folk': 742, 'night': 743, 'latest': 744, 'regul': 745, 'potenti': 746, 'default': 747, 'display': 748, 'soon': 749, 'cc': 750, 'break': 751, 'hardwar': 752, 'media': 753, 'lawrenc': 754, 'bitbitch': 755, 'taint': 756, 'compil': 757, 'adam': 758, 'letter': 759, 'william': 760, 'warn': 761, 'reserv': 762, 'object': 763, 'mass': 764, 'speak': 765, 'behalf': 766, 'abil': 767, 'male': 768, 'six': 769, 'unless': 770, 'concern': 771, 'hear': 772, 'invok': 773, 'earn': 774, 'lose': 775, 'sens': 776, 'model': 777, 'mention': 778, 'implement': 779, 'co': 780, 'pictur': 781, 'tie': 782, 'longer': 783, 'pgp': 784, 'whatev': 785, 'util': 786, 'short': 787, 'earli': 788, 'configur': 789, 'gari': 790, 'id': 791, 'replac': 792, 'especi': 793, 'spec': 794, 'father': 795, 'third': 796, 'kid': 797, 'numberth': 798, 'function': 799, 'procedur': 800, 'variou': 801, 'k': 802, 'scienc': 803, 'heard': 804, 'q': 805, 'qualiti': 806, 'advantag': 807, 'switch': 808, 'guid': 809, 'depart': 810, 'speech': 811, 'recommend': 812, 'insid': 813, 'produc': 814, 'indic': 815, 'grant': 816, 'oh': 817, 'locat': 818, 'predict': 819, 'will': 820, 'win': 821, 'togeth': 822, 'toni': 823, 'definit': 824, 'music': 825, 'appar': 826, 'washington': 827, 'button': 828, 'neg': 829, 'safe': 830, 'numberk': 831, 'movi': 832, 'took': 833, 'tuesday': 834, 'pc': 835, 'approach': 836, 'readi': 837, 'xml': 838, 'handl': 839, 'independ': 840, 'count': 841, 'rest': 842, 'attempt': 843, 'coupl': 844, 'valuabl': 845, 'format': 846, 'built': 847, 'popul': 848, 'certainli': 849, 'enter': 850, 'taken': 851, 'accord': 852, 'educ': 853, 'review': 854, 'section': 855, 'histori': 856, 'clue': 857, 'white': 858, 'spamd': 859, 'argument': 860, 'common': 861, 'dear': 862, 'consum': 863, 'modifi': 864, 'tom': 865, 'polic': 866, 'death': 867, 'michael': 868, 'mode': 869, 'quick': 870, 'rh': 871, 'disk': 872, 'south': 873, 'young': 874, 'deserv': 875, 'murphi': 876, 'bring': 877, 'path': 878, 'air': 879, 'four': 880, 'laptop': 881, 'statu': 882, 'expens': 883, 'traffic': 884, 'mime': 885, 'event': 886, 'initi': 887, 'block': 888, 'sometim': 889, 'choos': 890, 'empir': 891, 'knowledg': 892, 'automat': 893, 'fulli': 894, 'economi': 895, 'pack': 896, 'dr': 897, 'billion': 898, 'purpos': 899, 'cheer': 900, 'period': 901, 'patch': 902, 'backup': 903, 'membership': 904, 'v': 905, 'multipl': 906, 'compar': 907, 'class': 908, 'host': 909, 'declin': 910, 'dead': 911, 'promot': 912, 'field': 913, 'energi': 914, 'french': 915, 'concentr': 916, 'troubl': 917, 'promis': 918, 'partit': 919, 'children': 920, 'corpu': 921, 'capabl': 922, 'defin': 923, 'china': 924, 'franc': 925, 'evid': 926, 'popular': 927, 'photo': 928, 'agenc': 929, 'im': 930, 'huge': 931, 'en': 932, 'david': 933, 'cover': 934, 'geeg': 935, 'employ': 936, 'freedom': 937, 'team': 938, 'art': 939, 'materi': 940, 'awar': 941, 'black': 942, 'wireless': 943, 'health': 944, 'cool': 945, 'avoid': 946, 'classifi': 947, 'moment': 948, 'directli': 949, 'pudg': 950, 'imagin': 951, 'app': 952, 'foreign': 953, 'sum': 954, 'critic': 955, 'tue': 956, 'effici': 957, 'strategi': 958, 'extrem': 959, 'west': 960, 'src': 961, 'frnumber': 962, 'video': 963, 'whose': 964, 'fight': 965, 'director': 966, 'lower': 967, 'fee': 968, 'dream': 969, 'norton': 970, 'rebuild': 971, 'owner': 972, 'solv': 973, 'higher': 974, 'boston': 975, 'shop': 976, 'outsid': 977, 'pull': 978, 'valid': 979, 'middl': 980, 'fastest': 981, 'jm': 982, 'adult': 983, 'stand': 984, 'dan': 985, 'uniqu': 986, 'congress': 987, 'cabl': 988, 'canada': 989, 'player': 990, 'fun': 991, 'near': 992, 'programm': 993, 'anywher': 994, 'europ': 995, 'jeremi': 996, 'measur': 997, 'investig': 998, 'sender': 999, 'vs': 1000}\n"
     ]
    }
   ],
   "source": [
    "#Now applying the 2 functions written above to transform all the email data\n",
    "from sklearn.pipeline import Pipeline\n",
    "\n",
    "preprocess_pipeline=Pipeline([\n",
    "    ('email_to_word_count',EmailToWordCounterTransform()),\n",
    "    ('wordcount_to_vector',WordCounterToVectorTransform()),\n",
    "])\n",
    "\n",
    "X_train_transformed=preprocess_pipeline.fit_transform(X_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "id": "20cd3633-62b2-41d8-ac49-e89b3fdde1ac",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV] END ................................ score: (test=0.882) total time=   0.2s\n",
      "[CV] END ................................ score: (test=0.882) total time=   0.2s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=1)]: Done   1 out of   1 | elapsed:    0.3s remaining:    0.0s\n",
      "[Parallel(n_jobs=1)]: Done   2 out of   2 | elapsed:    0.4s remaining:    0.0s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV] END ................................ score: (test=0.897) total time=   0.1s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=1)]: Done   3 out of   3 | elapsed:    0.5s finished\n"
     ]
    }
   ],
   "source": [
    "#Model the preprocessed data using Logistic Regression\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.model_selection import cross_val_score\n",
    "\n",
    "spam_clf=LogisticRegression(solver='liblinear',random_state=42)\n",
    "score=cross_val_score(spam_clf,X_train_transformed,y_train,cv=3,verbose=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "id": "81268f65-4d47-41e6-8169-cb482094dfab",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.8874999999999998"
      ]
     },
     "execution_count": 112,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "score.mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "id": "0cf790bf-c848-4280-93a3-0171f596f1b1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Precision: 89.92%\n",
      "Recall: 96.27%\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import precision_score, recall_score\n",
    "\n",
    "X_test_transformed = preprocess_pipeline.transform(X_test)\n",
    "\n",
    "log_clf = LogisticRegression(solver=\"liblinear\", random_state=42)\n",
    "log_clf.fit(X_train_transformed, y_train)\n",
    "\n",
    "y_pred = log_clf.predict(X_test_transformed)\n",
    "\n",
    "print(\"Precision: {:.2f}%\".format(100 * precision_score(y_test, y_pred)))\n",
    "print(\"Recall: {:.2f}%\".format(100 * recall_score(y_test, y_pred)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "id": "3c4224a0-e0be-44fe-913f-6455e99f411c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[463,  19],\n",
       "       [ 51,  67]])"
      ]
     },
     "execution_count": 96,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.metrics import confusion_matrix\n",
    "\n",
    "confusion_matrix(y_test, y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "id": "0c5ab509-7faf-44be-b666-b9967378d36c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV] END ................................ score: (test=0.841) total time=   2.6s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=1)]: Done   1 out of   1 | elapsed:    2.6s remaining:    0.0s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV] END ................................ score: (test=0.841) total time=   3.7s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=1)]: Done   2 out of   2 | elapsed:    6.4s remaining:    0.0s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV] END ................................ score: (test=0.840) total time=   3.8s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=1)]: Done   3 out of   3 | elapsed:   10.2s finished\n"
     ]
    }
   ],
   "source": [
    "#Model the preprocessed data using SVM\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.model_selection import cross_val_score\n",
    "\n",
    "svc_clf=SVC()\n",
    "score=cross_val_score(svc_clf,X_train_transformed,y_train,cv=3,verbose=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "id": "cb4a6dfe-8e1b-433a-b70d-ffb7114d6dfa",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.8408333333333333"
      ]
     },
     "execution_count": 140,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "score.mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "id": "86bf6a22-14fd-46bd-99ea-465e2e1c8bfd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Precision: 80.33%\n",
      "Recall: 100.00%\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import precision_score, recall_score\n",
    "\n",
    "svm_clf = SVC()\n",
    "svm_clf.fit(X_train_transformed, y_train)\n",
    "\n",
    "y_pred_svm = svm_clf.predict(X_test_transformed)\n",
    "\n",
    "print(\"Precision: {:.2f}%\".format(100 * precision_score(y_test, y_pred_svm)))\n",
    "print(\"Recall: {:.2f}%\".format(100 * recall_score(y_test, y_pred_svm)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "id": "3dc6f871-c757-4b92-ace0-d0589c424995",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[  0, 118],\n",
       "       [  0, 482]])"
      ]
     },
     "execution_count": 142,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.metrics import confusion_matrix\n",
    "\n",
    "confusion_matrix(y_test, y_pred_svm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "id": "84e3e5eb-975f-4a67-afc0-f2a1383b0d8c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1 1 1 0 1 1 1 1 0 1 1 0 1 1 1 1 1 0 1 1 0 0 1 1 1 1 1 1 0 1 1 1 1 1 0 1 1\n",
      " 0 0 1 0 0 0 1 1 1 1 0 1 1 1 1 0 1 0 1 1 1 1 1 1 1 1 0 0 1 1 1 1 1 1 1 1 1\n",
      " 0 1 1 1 1 1 1 1 1 1 1 0 1 1 1 1 0 1 1 1 1 0 1 1 0 1 1 0 1 1 1 1 1 0 0 1 1\n",
      " 1 1 1 0 1 0 0 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 1 1 1 1 1 1 1 1 0 1 1\n",
      " 1 0 1 1 1 1 0 1 0 1 1 1 1 1 0 1 1 1 0 1 1 1 1 1 1 1 1 1 1 1 1 1 0 1 1 1 0\n",
      " 0 0 1 1 1 1 1 1 0 1 1 0 1 1 0 1 1 0 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 1 1 1 1\n",
      " 1 1 1 0 0 1 1 0 1 1 1 0 1 1 1 1 1 1 1 1 1 1 0 0 1 0 1 1 1 1 0 1 1 1 1 1 1\n",
      " 1 1 1 0 1 1 1 1 1 1 0 1 1 1 1 1 1 1 1 1 0 1 1 0 1 1 1 0 1 0 1 1 1 1 1 1 1\n",
      " 1 1 1 0 0 1 0 1 1 1 1 1 1 0 0 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 1 1 1 1 1 1\n",
      " 1 1 1 1 1 1 1 1 0 1 1 1 1 1 1 1 0 1 1 0 1 1 0 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
      " 0 1 1 1 0 1 1 1 0 1 1 1 1 1 0 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0\n",
      " 1 1 1 1 0 1 0 1 1 1 0 1 0 0 1 1 1 1 1 1 0 1 0 1 0 1 1 1 1 1 1 1 1 0 1 1 1\n",
      " 1 0 1 1 0 1 1 1 1 1 1 0 1 0 0 0 1 0 0 0 1 1 1 1 0 1 1 1 1 0 1 1 1 1 1 1 1\n",
      " 0 1 1 1 1 1 1 0 1 1 1 1 1 1 1 1 1 0 1 1 1 1 1 0 1 1 1 1 1 1 0 1 1 1 1 1 1\n",
      " 1 1 0 1 1 1 1 1 1 1 1 1 1 1 1 1 0 1 1 1 1 1 1 1 0 1 1 1 0 1 1 1 1 1 0 1 0\n",
      " 1 1 0 0 1 1 1 1 1 0 1 1 1 1 1 1 0 1 1 1 0 1 1 1 0 0 0 1 1 0 1 1 1 1 1 1 1\n",
      " 1 1 1 0 1 1 1 1]\n"
     ]
    }
   ],
   "source": [
    "#print(y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d5ebc59d-10d9-48b1-bae5-112baad99cd0",
   "metadata": {},
   "outputs": [],
   "source": [
    "#print(y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "44a7ac3c-a042-4e01-9547-fed3947e686d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "43856969-2af8-4031-ac69-e2f5d1d32b3a",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
